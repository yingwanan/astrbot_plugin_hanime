import aiohttp
import asyncio
from bs4 import BeautifulSoup
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
from astrbot.api.message_components import Plain, Image

@register("astrbot_plugin_hanime", "YourName", "Hanimeæœç´¢æ’ä»¶", "1.0.1")
class HanimePlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        # é…ç½®ï¼šæœç´¢ç»“æœæ˜¾ç¤ºæ•°é‡ (å»ºè®®ä¸è¦å¤ªå¤§ï¼Œå› ä¸ºç°åœ¨ä¼šé¢„åŠ è½½è¯¦æƒ…é¡µ)
        self.max_results = 5 
        self.search_cache = {} 
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
            "Referer": "https://hanime1.me/" # åŠ ä¸Š Referer é˜²ç›—é“¾
        }

    async def _fetch_video_detail(self, session, url, idx):
        """
        è¾…åŠ©å‡½æ•°ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œæå–é«˜æ¸…å°é¢å’Œç¡®åˆ‡æ ‡é¢˜
        è¿”å›: (index, data_dict) æˆ– None
        """
        try:
            async with session.get(url, headers=self.headers, timeout=10) as resp:
                if resp.status != 200:
                    return None
                html = await resp.text()
                soup = BeautifulSoup(html, "lxml")
                
                # æå–æ ‡é¢˜ (og:title é€šå¸¸æœ€å‡†ç¡®)
                og_title = soup.find("meta", property="og:title")
                title = og_title["content"] if og_title else "æœªçŸ¥æ ‡é¢˜"
                
                # æå–å°é¢ (ä» video poster å±æ€§è·å–ï¼Œè¿™æ˜¯æœ€é«˜æ¸…ä¸”çœŸå®çš„å°é¢)
                video_tag = soup.find("video", id="player")
                cover_url = ""
                if video_tag and video_tag.has_attr("poster"):
                    cover_url = video_tag["poster"]
                
                # å¦‚æœæ²¡æœ‰ posterï¼Œå°è¯• og:image
                if not cover_url:
                    og_image = soup.find("meta", property="og:image")
                    if og_image:
                        cover_url = og_image["content"]

                return idx, {
                    "title": title,
                    "url": url,
                    "cover_url": cover_url
                }
        except Exception as e:
            logger.error(f"Parse detail error: {e}")
            return None

    # ---------------- æŒ‡ä»¤: æœç´¢ (/lf) ----------------
    @filter.command("lf")
    async def search_hanime(self, event: AstrMessageEvent, keyword: str):
        """æœç´¢ Hanime1: /lf <å…³é”®è¯>"""
        if not keyword:
            yield event.plain_result("è¯·è¾“å…¥å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š/lf æŸä¸ªç•ªå‰§")
            return

        yield event.plain_result(f"ğŸ” æ­£åœ¨æœç´¢ '{keyword}' å¹¶è§£æå°é¢ï¼Œè¯·ç¨å€™...")

        search_url = f"https://hanime1.me/search?query={keyword}"
        
        try:
            async with aiohttp.ClientSession() as session:
                # 1. è·å–æœç´¢åˆ—è¡¨
                async with session.get(search_url, headers=self.headers) as resp:
                    if resp.status != 200:
                        yield event.plain_result(f"è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status}")
                        return
                    html = await resp.text()

                # 2. åˆæ­¥è§£æåˆ—è¡¨
                soup = BeautifulSoup(html, "lxml")
                results_div = soup.find("div", class_="content-padding-new")
                if not results_div:
                     yield event.plain_result("æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
                     return

                # è·å–æ‰€æœ‰å¯èƒ½çš„æ¡ç›®
                raw_items = results_div.find_all("div", class_="col-xs-6")
                
                candidate_urls = []
                
                # 3. ç­›é€‰å‡ºçœŸæ­£çš„è§†é¢‘é“¾æ¥ (è¿‡æ»¤å¹¿å‘Š)
                for item in raw_items:
                    if len(candidate_urls) >= self.max_results:
                        break
                        
                    a_tag = item.find("a", class_="overlay")
                    if not a_tag:
                        continue
                    
                    href = a_tag.get("href")
                    # æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ï¼šå¿…é¡»åŒ…å« /watch?v= æ‰æ˜¯æ­£ç‰‡ï¼Œå¹¿å‘Šé€šå¸¸æ²¡æœ‰è¿™ä¸ªç‰¹å¾
                    if not href or "/watch?v=" not in href:
                        continue
                        
                    if not href.startswith("http"):
                        href = "https://hanime1.me" + href
                    
                    candidate_urls.append(href)

                if not candidate_urls:
                    yield event.plain_result("æœªæ‰¾åˆ°ç›¸å…³è§†é¢‘ (å·²è¿‡æ»¤å¹¿å‘Š)ã€‚")
                    return

                # 4. å¹¶å‘è¯·æ±‚è¯¦æƒ…é¡µ (ä¸ºäº†è·å–æ­£ç¡®çš„å°é¢å›¾)
                tasks = []
                for i, url in enumerate(candidate_urls):
                    tasks.append(self._fetch_video_detail(session, url, i))
                
                # ç­‰å¾…æ‰€æœ‰è¯¦æƒ…é¡µè§£æå®Œæˆ
                details_results = await asyncio.gather(*tasks)
                
                # æ•´ç†ç»“æœ
                valid_items = []
                for res in details_results:
                    if res:
                        valid_items.append(res[1]) # data_dict
                
                if not valid_items:
                    yield event.plain_result("è§£æè§†é¢‘è¯¦æƒ…å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                    return

                # 5. ç¼“å­˜å¹¶å‘é€æ¶ˆæ¯
                user_id = event.get_sender_id()
                self.search_cache[user_id] = valid_items
                
                msg_chain = [Plain(f"âœ¨ å…³é”®è¯ '{keyword}' æœç´¢ç»“æœ:\n")]

                for idx, data in enumerate(valid_items):
                    title = data["title"]
                    cover = data["cover_url"]
                    
                    msg_chain.append(Plain(f"\n{idx + 1}. {title}\n"))
                    if cover:
                        msg_chain.append(Image.fromURL(cover))
                
                msg_chain.append(Plain("\nğŸ’¡ å‘é€ /lfxz <ç¼–å·> è·å–è§†é¢‘ç›´é“¾"))
                yield event.chain_result(msg_chain)

        except Exception as e:
            logger.error(f"Search error: {e}")
            yield event.plain_result(f"å‘ç”Ÿé”™è¯¯: {str(e)}")

    # ---------------- æŒ‡ä»¤: é€‰é›† (/lfxz) ----------------
    @filter.command("lfxz")
    async def select_video(self, event: AstrMessageEvent, index: str):
        """è·å–è§†é¢‘ç›´é“¾: /lfxz <ç¼–å·>"""
        user_id = event.get_sender_id()
        
        if user_id not in self.search_cache or not self.search_cache[user_id]:
            yield event.plain_result("è¯·å…ˆä½¿ç”¨ /lf <å…³é”®è¯> è¿›è¡Œæœç´¢ã€‚")
            return

        if not index.isdigit():
            yield event.plain_result("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
            return
        
        idx = int(index) - 1
        if idx < 0 or idx >= len(self.search_cache[user_id]):
            yield event.plain_result("ç¼–å·è¶…å‡ºèŒƒå›´ã€‚")
            return

        target = self.search_cache[user_id][idx]
        detail_url = target["url"]
        title = target["title"]

        yield event.plain_result(f"æ­£åœ¨è§£æ '{title}' ç›´é“¾...")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(detail_url, headers=self.headers) as resp:
                    if resp.status != 200:
                        yield event.plain_result("æ— æ³•è®¿é—®è§†é¢‘è¯¦æƒ…é¡µã€‚")
                        return
                    html = await resp.text()
        except Exception as e:
            yield event.plain_result(f"ç½‘ç»œé”™è¯¯: {e}")
            return

        soup = BeautifulSoup(html, "lxml")
        video_tag = soup.find("video", id="player")
        
        video_src = ""
        if video_tag:
            # ä¼˜å…ˆæ‰¾ source æ ‡ç­¾
            source_tag = video_tag.find("source")
            if source_tag:
                video_src = source_tag.get("src")
        
        # å…œåº•æ­£åˆ™æŸ¥æ‰¾
        if not video_src:
            import re
            match = re.search(r'https?://[^\s"\']+\.m3u8', html)
            if match:
                video_src = match.group(0)

        if video_src:
            # å‘é€ç›´é“¾
            yield event.plain_result(f"ğŸ¬ {title}\n\nç›´é“¾åœ°å€:\n{video_src}\n\n(å¤åˆ¶é“¾æ¥åˆ°æµè§ˆå™¨æˆ–ä¸‹è½½å™¨å³å¯è§‚çœ‹/ä¸‹è½½)")
        else:
            yield event.plain_result("æœªè§£æåˆ°è§†é¢‘ç›´é“¾ï¼Œè¯·é‡è¯•æˆ–æ›´æ¢è§†é¢‘ã€‚")
