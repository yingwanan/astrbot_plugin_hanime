import re
import aiohttp
import asyncio
from collections import OrderedDict
from bs4 import BeautifulSoup
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star
from astrbot.api import logger
from astrbot.api.message_components import Plain, Image

class HanimePlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        # é…ç½®ï¼šæœç´¢ç»“æœæ˜¾ç¤ºæ•°é‡
        self.max_results = 5
        
        # ç¼“å­˜è®¾ç½®ï¼šæœ€å¤§ç¼“å­˜ç”¨æˆ·æ•°
        self.max_cache_size = 50
        # ä½¿ç”¨ OrderedDict å®ç° LRU ç¼“å­˜
        self.search_cache = OrderedDict()
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
            "Referer": "https://hanime1.me/"
        }
        self.session = None

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–æ—¶åˆ›å»ºå…¨å±€ Session"""
        self.session = aiohttp.ClientSession(headers=self.headers)
        logger.info(f"[{self.__class__.__name__}] ClientSession initialized.")

    async def terminate(self):
        """æ’ä»¶å¸è½½/åœæ­¢æ—¶å…³é—­ Session"""
        if self.session:
            await self.session.close()
            logger.info(f"[{self.__class__.__name__}] ClientSession closed.")

    def _update_cache(self, user_id, data):
        """æ›´æ–°ç¼“å­˜ï¼Œç»´æŠ¤ LRU ç­–ç•¥"""
        if user_id in self.search_cache:
            self.search_cache.move_to_end(user_id)
        self.search_cache[user_id] = data
        
        # å¦‚æœè¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›® (FIFO è¡Œä¸ºé…åˆ move_to_end å˜ä¸º LRU)
        if len(self.search_cache) > self.max_cache_size:
            self.search_cache.popitem(last=False)

    async def _fetch_video_detail(self, url, idx):
        """
        è¾…åŠ©å‡½æ•°ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œæå–é«˜æ¸…å°é¢å’Œç¡®åˆ‡æ ‡é¢˜
        """
        if not self.session:
            return None

        try:
            # è®¾ç½®è¶…æ—¶é˜²æ­¢å¡é¡¿
            async with self.session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return None
                html = await resp.text()
                soup = BeautifulSoup(html, "lxml")
                
                # æå–æ ‡é¢˜
                og_title = soup.find("meta", property="og:title")
                title = og_title["content"] if og_title else "æœªçŸ¥æ ‡é¢˜"
                
                # æå–å°é¢ (ä¼˜å…ˆ poster)
                video_tag = soup.find("video", id="player")
                cover_url = ""
                if video_tag and video_tag.has_attr("poster"):
                    cover_url = video_tag["poster"]
                
                # å…œåº•æå–å°é¢
                if not cover_url:
                    og_image = soup.find("meta", property="og:image")
                    if og_image:
                        cover_url = og_image["content"]

                return idx, {
                    "title": title,
                    "url": url,
                    "cover_url": cover_url
                }
        except Exception as e:
            logger.error(f"Parse detail error for {url}: {e}")
            return None

    # ---------------- æŒ‡ä»¤: æœç´¢ (/lf) ----------------
    @filter.command("lf")
    async def search_hanime(self, event: AstrMessageEvent, keyword: str):
        """æœç´¢ Hanime1: /lf <å…³é”®è¯>"""
        if not keyword:
            yield event.plain_result("è¯·è¾“å…¥å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š/lf æŸä¸ªç•ªå‰§")
            return
        
        if not self.session:
            # æå°‘è§çš„æƒ…å†µï¼Œé˜²æ­¢åˆå§‹åŒ–å¤±è´¥å¯¼è‡´ crash
            self.session = aiohttp.ClientSession(headers=self.headers)

        yield event.plain_result(f"ğŸ” æ­£åœ¨æœç´¢ '{keyword}' å¹¶è§£æå°é¢ï¼Œè¯·ç¨å€™...")

        search_url = f"https://hanime1.me/search?query={keyword}"
        
        try:
            # 1. è·å–æœç´¢åˆ—è¡¨
            async with self.session.get(search_url) as resp:
                if resp.status != 200:
                    yield event.plain_result(f"è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status}")
                    return
                html = await resp.text()

            # 2. åˆæ­¥è§£æåˆ—è¡¨
            soup = BeautifulSoup(html, "lxml")
            results_div = soup.find("div", class_="content-padding-new")
            if not results_div:
                 yield event.plain_result("æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
                 return

            raw_items = results_div.find_all("div", class_="col-xs-6")
            candidate_urls = []
            
            # 3. ç­›é€‰é“¾æ¥ (è¿‡æ»¤å¹¿å‘Š)
            for item in raw_items:
                if len(candidate_urls) >= self.max_results:
                    break
                    
                a_tag = item.find("a", class_="overlay")
                if not a_tag:
                    continue
                
                href = a_tag.get("href")
                # æ ¸å¿ƒè¿‡æ»¤ï¼šå¿…é¡»åŒ…å« /watch?v=
                if not href or "/watch?v=" not in href:
                    continue
                    
                if not href.startswith("http"):
                    href = "https://hanime1.me" + href
                
                candidate_urls.append(href)

            if not candidate_urls:
                yield event.plain_result("æœªæ‰¾åˆ°ç›¸å…³è§†é¢‘ (å·²è¿‡æ»¤å¹¿å‘Š)ã€‚")
                return

            # 4. å¹¶å‘é¢„åŠ è½½è¯¦æƒ…é¡µ
            tasks = []
            for i, url in enumerate(candidate_urls):
                tasks.append(self._fetch_video_detail(url, i))
            
            details_results = await asyncio.gather(*tasks)
            
            valid_items = []
            for res in details_results:
                if res:
                    valid_items.append(res[1])
            
            if not valid_items:
                yield event.plain_result("è§£æè§†é¢‘è¯¦æƒ…å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                return

            # 5. æ›´æ–°ç¼“å­˜ (LRU)
            user_id = event.get_sender_id()
            self._update_cache(user_id, valid_items)
            
            # 6. æ„å»ºæ¶ˆæ¯
            msg_chain = [Plain(f"âœ¨ å…³é”®è¯ '{keyword}' æœç´¢ç»“æœ:\n")]
            for idx, data in enumerate(valid_items):
                title = data["title"]
                cover = data["cover_url"]
                
                msg_chain.append(Plain(f"\n{idx + 1}. {title}\n"))
                if cover:
                    msg_chain.append(Image.fromURL(cover))
            
            msg_chain.append(Plain("\nğŸ’¡ å‘é€ /lfxz <ç¼–å·> è·å–è§†é¢‘ç›´é“¾"))
            yield event.chain_result(msg_chain)

        except Exception as e:
            logger.error(f"Search error: {e}")
            yield event.plain_result(f"å‘ç”Ÿé”™è¯¯: {str(e)}")

    # ---------------- æŒ‡ä»¤: é€‰é›† (/lfxz) ----------------
    @filter.command("lfxz")
    async def select_video(self, event: AstrMessageEvent, index: str):
        """è·å–è§†é¢‘ç›´é“¾: /lfxz <ç¼–å·>"""
        user_id = event.get_sender_id()
        
        # æ£€æŸ¥ç¼“å­˜
        if user_id not in self.search_cache:
            yield event.plain_result("è¯·å…ˆä½¿ç”¨ /lf <å…³é”®è¯> è¿›è¡Œæœç´¢ã€‚")
            return

        # åˆ·æ–° LRU ä½ç½®
        self.search_cache.move_to_end(user_id)
        items = self.search_cache[user_id]

        if not index.isdigit():
            yield event.plain_result("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
            return
        
        idx = int(index) - 1
        if idx < 0 or idx >= len(items):
            yield event.plain_result("ç¼–å·è¶…å‡ºèŒƒå›´ã€‚")
            return

        target = items[idx]
        detail_url = target["url"]
        title = target["title"]

        yield event.plain_result(f"æ­£åœ¨è§£æ '{title}' ç›´é“¾...")

        try:
            if not self.session:
                self.session = aiohttp.ClientSession(headers=self.headers)

            async with self.session.get(detail_url) as resp:
                if resp.status != 200:
                    yield event.plain_result("æ— æ³•è®¿é—®è§†é¢‘è¯¦æƒ…é¡µã€‚")
                    return
                html = await resp.text()
        except Exception as e:
            yield event.plain_result(f"ç½‘ç»œé”™è¯¯: {e}")
            return

        soup = BeautifulSoup(html, "lxml")
        video_tag = soup.find("video", id="player")
        
        video_src = ""
        if video_tag:
            source_tag = video_tag.find("source")
            if source_tag:
                video_src = source_tag.get("src")
        
        if not video_src:
            # æ­£åˆ™å·²ç§»è‡³é¡¶éƒ¨å¼•ç”¨
            match = re.search(r'https?://[^\s"\']+\.m3u8', html)
            if match:
                video_src = match.group(0)

        if video_src:
            yield event.plain_result(f"ğŸ¬ {title}\n\nç›´é“¾åœ°å€:\n{video_src}\n\n(å¤åˆ¶é“¾æ¥åˆ°æµè§ˆå™¨æˆ–ä¸‹è½½å™¨å³å¯è§‚çœ‹/ä¸‹è½½)")
        else:
            yield event.plain_result("æœªè§£æåˆ°è§†é¢‘ç›´é“¾ï¼Œè¯·é‡è¯•æˆ–æ›´æ¢è§†é¢‘ã€‚")
